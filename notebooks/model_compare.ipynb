{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# CNN vs ViT ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ\n",
        "\n",
        "ì´ ë…¸íŠ¸ë¶ì—ì„œëŠ” CNNê³¼ ViT ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ë¹„êµí•˜ê³  ë¶„ì„í•©ë‹ˆë‹¤.\n",
        "\n",
        "## ëª©í‘œ\n",
        "- CNNê³¼ ViT ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ\n",
        "- ë©”íŠ¸ë¦­ ë¹„êµ (Accuracy, F1-score, Precision, Recall)\n",
        "- Loss Curve ë¹„êµ\n",
        "- Confusion Matrix ë¹„êµ\n",
        "- ì˜¤ë¶„ë¥˜ ìƒ˜í”Œ ë¹„êµ ë¶„ì„\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ import\n",
        "import sys\n",
        "sys.path.insert(0, '../src')\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import json\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "\n",
        "# í”„ë¡œì íŠ¸ ëª¨ë“ˆ import\n",
        "from models.model_utils import create_cnn_model, create_vit_model, load_model_from_checkpoint\n",
        "from data.dataset import ImageDataset\n",
        "from data.preprocess import get_val_transforms\n",
        "from training.evaluator import evaluate_model\n",
        "from training.metrics import plot_confusion_matrix\n",
        "\n",
        "print(\"âœ… ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ ì™„ë£Œ!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ì„¤ì • ë° ë°ì´í„° ì¤€ë¹„\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ê²½ë¡œ ì„¤ì •\n",
        "DATA_DIR = Path(\"../data\")\n",
        "TEST_DIR = DATA_DIR / \"test\"\n",
        "CHECKPOINT_DIR = Path(\"../experiments/checkpoints\")\n",
        "RESULTS_DIR = Path(\"../experiments/results\")\n",
        "LOG_DIR = Path(\"../experiments/logs\")\n",
        "\n",
        "# ê²°ê³¼ ë””ë ‰í† ë¦¬ ìƒì„±\n",
        "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# ì„¤ì •\n",
        "CONFIG = {\n",
        "    'image_size': 224,\n",
        "    'batch_size': 32,\n",
        "    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
        "    'class_names': ['real', 'fake'],\n",
        "    'class_map': {'real': '0', 'fake': '1'}\n",
        "}\n",
        "\n",
        "# ë¹„êµí•  ëª¨ë¸ ì„¤ì •\n",
        "MODELS_TO_COMPARE = {\n",
        "    'CNN_ResNet18': {\n",
        "        'type': 'cnn',\n",
        "        'model_name': 'resnet18',\n",
        "        'checkpoint': CHECKPOINT_DIR / 'CNN_resnet18_best.pth'\n",
        "    },\n",
        "    'ViT_Base': {\n",
        "        'type': 'vit',\n",
        "        'model_name': 'vit_base',\n",
        "        'checkpoint': CHECKPOINT_DIR / 'ViT_vit_base_best.pth'\n",
        "    }\n",
        "}\n",
        "\n",
        "print(f\"ë””ë°”ì´ìŠ¤: {CONFIG['device']}\")\n",
        "print(f\"ë¹„êµí•  ëª¨ë¸: {list(MODELS_TO_COMPARE.keys())}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ì¤€ë¹„\n",
        "test_transform = get_val_transforms(CONFIG['image_size'])\n",
        "test_dataset = ImageDataset(TEST_DIR, CONFIG['class_map'], transform=test_transform)\n",
        "test_loader = DataLoader(test_dataset, batch_size=CONFIG['batch_size'], shuffle=False, num_workers=4)\n",
        "\n",
        "print(f\"í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(test_dataset)}ê°œ\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ëª¨ë¸ ë¡œë“œ ë° í‰ê°€\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ê° ëª¨ë¸ í‰ê°€ ë° ê²°ê³¼ ì €ì¥\n",
        "results = {}\n",
        "histories = {}\n",
        "\n",
        "for model_display_name, model_config in MODELS_TO_COMPARE.items():\n",
        "    print(f\"\\\\n{'='*70}\")\n",
        "    print(f\"ëª¨ë¸ í‰ê°€: {model_display_name}\")\n",
        "    print(f\"{'='*70}\")\n",
        "    \n",
        "    checkpoint_path = model_config['checkpoint']\n",
        "    \n",
        "    if not checkpoint_path.exists():\n",
        "        print(f\"âš ï¸  ì²´í¬í¬ì¸íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {checkpoint_path}\")\n",
        "        continue\n",
        "    \n",
        "    # ëª¨ë¸ ë¡œë“œ\n",
        "    model, checkpoint = load_model_from_checkpoint(\n",
        "        checkpoint_path=str(checkpoint_path),\n",
        "        model_type=model_config['type'],\n",
        "        model_name=model_config['model_name'],\n",
        "        num_classes=2,\n",
        "        device=CONFIG['device']\n",
        "    )\n",
        "    \n",
        "    # í‰ê°€\n",
        "    test_results = evaluate_model(\n",
        "        model,\n",
        "        test_loader,\n",
        "        CONFIG['device'],\n",
        "        class_names=CONFIG['class_names']\n",
        "    )\n",
        "    \n",
        "    # ê²°ê³¼ ì €ì¥\n",
        "    results[model_display_name] = {\n",
        "        'metrics': test_results['metrics'],\n",
        "        'predictions': test_results['predictions'],\n",
        "        'labels': test_results['labels']\n",
        "    }\n",
        "    \n",
        "    # íˆìŠ¤í† ë¦¬ ë¡œë“œ (ìˆëŠ” ê²½ìš°)\n",
        "    if 'history' in checkpoint:\n",
        "        histories[model_display_name] = checkpoint['history']\n",
        "    \n",
        "    print(f\"âœ… {model_display_name} í‰ê°€ ì™„ë£Œ\")\n",
        "    print(f\"   Accuracy: {test_results['metrics']['accuracy']:.4f}\")\n",
        "    print(f\"   F1 Score: {test_results['metrics']['f1']:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ì„±ëŠ¥ ë©”íŠ¸ë¦­ ë¹„êµ\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ë©”íŠ¸ë¦­ ë¹„êµ í…Œì´ë¸” ìƒì„±\n",
        "comparison_data = []\n",
        "for model_name, result in results.items():\n",
        "    metrics = result['metrics']\n",
        "    comparison_data.append({\n",
        "        'Model': model_name,\n",
        "        'Accuracy': metrics['accuracy'],\n",
        "        'Precision': metrics['precision'],\n",
        "        'Recall': metrics['recall'],\n",
        "        'F1 Score': metrics['f1']\n",
        "    })\n",
        "\n",
        "comparison_df = pd.DataFrame(comparison_data)\n",
        "comparison_df = comparison_df.set_index('Model')\n",
        "\n",
        "print(\"\\\\n\" + \"=\" * 70)\n",
        "print(\"ğŸ“Š ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ\")\n",
        "print(\"=\" * 70)\n",
        "print(comparison_df.round(4))\n",
        "\n",
        "# ê²°ê³¼ ì €ì¥\n",
        "comparison_df.to_csv(RESULTS_DIR / 'model_comparison.csv')\n",
        "comparison_df.to_json(RESULTS_DIR / 'model_comparison.json', indent=2)\n",
        "print(f\"\\\\nâœ… ë¹„êµ ê²°ê³¼ ì €ì¥: {RESULTS_DIR / 'model_comparison.csv'}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ë©”íŠ¸ë¦­ ì‹œê°í™” ë¹„êµ\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ë©”íŠ¸ë¦­ ë¹„êµ ë°” ì°¨íŠ¸\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "metrics_to_plot = ['accuracy', 'precision', 'recall', 'f1']\n",
        "metric_labels = ['Accuracy', 'Precision', 'Recall', 'F1 Score']\n",
        "\n",
        "for idx, (metric, label) in enumerate(zip(metrics_to_plot, metric_labels)):\n",
        "    ax = axes[idx // 2, idx % 2]\n",
        "    \n",
        "    model_names = list(results.keys())\n",
        "    metric_values = [results[name]['metrics'][metric] for name in model_names]\n",
        "    \n",
        "    bars = ax.bar(model_names, metric_values, color=['#3498db', '#e74c3c'][:len(model_names)])\n",
        "    ax.set_title(f'{label} ë¹„êµ', fontsize=12, fontweight='bold')\n",
        "    ax.set_ylabel(label)\n",
        "    ax.set_ylim([0, 1])\n",
        "    ax.grid(axis='y', alpha=0.3)\n",
        "    \n",
        "    # ê°’ í‘œì‹œ\n",
        "    for bar in bars:\n",
        "        height = bar.get_height()\n",
        "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
        "                f'{height:.3f}', ha='center', va='bottom', fontsize=10)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(RESULTS_DIR / 'model_comparison_metrics.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(f\"âœ… ë©”íŠ¸ë¦­ ë¹„êµ ê·¸ë˜í”„ ì €ì¥: {RESULTS_DIR / 'model_comparison_metrics.png'}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Loss Curve ë¹„êµ\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Loss Curve ë¹„êµ\n",
        "if histories:\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
        "    \n",
        "    # Loss Curve\n",
        "    for model_name, history in histories.items():\n",
        "        axes[0].plot(history['train_loss'], label=f'{model_name} Train', linestyle='--', alpha=0.7)\n",
        "        axes[0].plot(history['val_loss'], label=f'{model_name} Val', linewidth=2)\n",
        "    \n",
        "    axes[0].set_title('Loss Curve ë¹„êµ', fontsize=14, fontweight='bold')\n",
        "    axes[0].set_xlabel('Epoch')\n",
        "    axes[0].set_ylabel('Loss')\n",
        "    axes[0].legend()\n",
        "    axes[0].grid(True, alpha=0.3)\n",
        "    \n",
        "    # Accuracy Curve\n",
        "    for model_name, history in histories.items():\n",
        "        axes[1].plot(history['train_accuracy'], label=f'{model_name} Train', linestyle='--', alpha=0.7)\n",
        "        axes[1].plot(history['val_accuracy'], label=f'{model_name} Val', linewidth=2)\n",
        "    \n",
        "    axes[1].set_title('Accuracy Curve ë¹„êµ', fontsize=14, fontweight='bold')\n",
        "    axes[1].set_xlabel('Epoch')\n",
        "    axes[1].set_ylabel('Accuracy')\n",
        "    axes[1].legend()\n",
        "    axes[1].grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig(RESULTS_DIR / 'model_comparison_curves.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    \n",
        "    print(f\"âœ… Loss Curve ë¹„êµ ê·¸ë˜í”„ ì €ì¥: {RESULTS_DIR / 'model_comparison_curves.png'}\")\n",
        "else:\n",
        "    print(\"âš ï¸  íˆìŠ¤í† ë¦¬ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Confusion Matrix ë¹„êµ\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Confusion Matrix ë¹„êµ\n",
        "num_models = len(results)\n",
        "fig, axes = plt.subplots(1, num_models, figsize=(6*num_models, 5))\n",
        "\n",
        "if num_models == 1:\n",
        "    axes = [axes]\n",
        "\n",
        "for idx, (model_name, result) in enumerate(results.items()):\n",
        "    cm = result['metrics']['confusion_matrix']\n",
        "    \n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=CONFIG['class_names'],\n",
        "                yticklabels=CONFIG['class_names'],\n",
        "                ax=axes[idx])\n",
        "    axes[idx].set_title(f'{model_name}\\\\nConfusion Matrix', fontsize=12, fontweight='bold')\n",
        "    axes[idx].set_ylabel('True Label')\n",
        "    axes[idx].set_xlabel('Predicted Label')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(RESULTS_DIR / 'model_comparison_confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(f\"âœ… Confusion Matrix ë¹„êµ ì €ì¥: {RESULTS_DIR / 'model_comparison_confusion_matrix.png'}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ì˜¤ë¶„ë¥˜ ìƒ˜í”Œ ë¹„êµ ë¶„ì„\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ê° ëª¨ë¸ì˜ ì˜¤ë¶„ë¥˜ ìƒ˜í”Œ ì°¾ê¸°\n",
        "misclassified_by_model = {}\n",
        "\n",
        "for model_display_name, result in results.items():\n",
        "    predictions = np.array(result['predictions'])\n",
        "    labels = np.array(result['labels'])\n",
        "    \n",
        "    incorrect_indices = np.where(predictions != labels)[0]\n",
        "    misclassified_by_model[model_display_name] = incorrect_indices\n",
        "    \n",
        "    print(f\"{model_display_name}: {len(incorrect_indices)}ê°œ ì˜¤ë¶„ë¥˜ ({len(incorrect_indices)/len(labels)*100:.2f}%)\")\n",
        "\n",
        "# ê³µí†µ ì˜¤ë¶„ë¥˜ ìƒ˜í”Œ ì°¾ê¸°\n",
        "if len(misclassified_by_model) >= 2:\n",
        "    model_names = list(misclassified_by_model.keys())\n",
        "    common_misclassified = set(misclassified_by_model[model_names[0]])\n",
        "    for model_name in model_names[1:]:\n",
        "        common_misclassified = common_misclassified.intersection(set(misclassified_by_model[model_name]))\n",
        "    \n",
        "    print(f\"\\\\nê³µí†µ ì˜¤ë¶„ë¥˜ ìƒ˜í”Œ: {len(common_misclassified)}ê°œ\")\n",
        "    \n",
        "    # ê° ëª¨ë¸ë§Œ í‹€ë¦° ìƒ˜í”Œ\n",
        "    for model_name in model_names:\n",
        "        model_only = set(misclassified_by_model[model_name]) - common_misclassified\n",
        "        print(f\"{model_name}ë§Œ í‹€ë¦° ìƒ˜í”Œ: {len(model_only)}ê°œ\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ì¢…í•© ë¶„ì„ ë° ê²°ë¡ \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\" * 70)\n",
        "print(\"ğŸ“Š ëª¨ë¸ ë¹„êµ ì¢…í•© ë¶„ì„\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì°¾ê¸°\n",
        "best_model = comparison_df['F1 Score'].idxmax()\n",
        "best_accuracy = comparison_df.loc[best_model, 'Accuracy']\n",
        "best_f1 = comparison_df.loc[best_model, 'F1 Score']\n",
        "\n",
        "print(f\"\\\\nğŸ† ìµœê³  ì„±ëŠ¥ ëª¨ë¸: {best_model}\")\n",
        "print(f\"   Accuracy: {best_accuracy:.4f} ({best_accuracy*100:.2f}%)\")\n",
        "print(f\"   F1 Score: {best_f1:.4f}\")\n",
        "\n",
        "print(f\"\\\\nğŸ“ˆ ëª¨ë¸ë³„ íŠ¹ì§•:\")\n",
        "for model_name in results.keys():\n",
        "    metrics = results[model_name]['metrics']\n",
        "    print(f\"\\\\n{model_name}:\")\n",
        "    print(f\"  - Accuracy:  {metrics['accuracy']:.4f} ({metrics['accuracy']*100:.2f}%)\")\n",
        "    print(f\"  - Precision: {metrics['precision']:.4f}\")\n",
        "    print(f\"  - Recall:    {metrics['recall']:.4f}\")\n",
        "    print(f\"  - F1 Score:  {metrics['f1']:.4f}\")\n",
        "\n",
        "print(f\"\\\\nğŸ’¡ ê²°ë¡ :\")\n",
        "if 'CNN' in best_model and 'ViT' in comparison_df.index:\n",
        "    cnn_f1 = comparison_df.loc[comparison_df.index.str.contains('CNN')[0], 'F1 Score'] if any(comparison_df.index.str.contains('CNN')) else None\n",
        "    vit_f1 = comparison_df.loc[comparison_df.index.str.contains('ViT')[0], 'F1 Score'] if any(comparison_df.index.str.contains('ViT')) else None\n",
        "    if cnn_f1 is not None and vit_f1 is not None:\n",
        "        if cnn_f1 > vit_f1:\n",
        "            print(\"  - CNN ëª¨ë¸ì´ ì´ ë°ì´í„°ì…‹ì—ì„œ ë” ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤.\")\n",
        "        elif vit_f1 > cnn_f1:\n",
        "            print(\"  - ViT ëª¨ë¸ì´ ì´ ë°ì´í„°ì…‹ì—ì„œ ë” ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤.\")\n",
        "        else:\n",
        "            print(\"  - ë‘ ëª¨ë¸ì˜ ì„±ëŠ¥ì´ ë¹„ìŠ·í•©ë‹ˆë‹¤.\")\n",
        "\n",
        "print(f\"\\\\nğŸ“ ì €ì¥ëœ íŒŒì¼:\")\n",
        "print(f\"  - ë¹„êµ í…Œì´ë¸”: {RESULTS_DIR / 'model_comparison.csv'}\")\n",
        "print(f\"  - ë©”íŠ¸ë¦­ ë¹„êµ: {RESULTS_DIR / 'model_comparison_metrics.png'}\")\n",
        "print(f\"  - Loss Curve ë¹„êµ: {RESULTS_DIR / 'model_comparison_curves.png'}\")\n",
        "print(f\"  - Confusion Matrix ë¹„êµ: {RESULTS_DIR / 'model_comparison_confusion_matrix.png'}\")\n",
        "print(\"=\" * 70)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
