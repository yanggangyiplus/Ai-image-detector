{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ì „ì²˜ë¦¬ ì‹¤í—˜\n",
        "\n",
        "ì´ ë…¸íŠ¸ë¶ì—ì„œëŠ” ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ê¸°ë²•ì„ ì‹¤í—˜í•©ë‹ˆë‹¤.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ import\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from pathlib import Path\n",
        "import shutil\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "import re\n",
        "from sklearn.preprocessing import StandardScaler\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ì„¤ì •\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ê²½ë¡œ ì„¤ì •\n",
        "RAW_DATA_DIR = Path(\"../data/raw\")\n",
        "PROCESSED_DATA_DIR = Path(\"../data/processed\")\n",
        "\n",
        "# ì „ì²˜ë¦¬ ì„¤ì •\n",
        "IMAGE_SIZE = (224, 224)  # ì´ë¯¸ì§€ ë¦¬ì‚¬ì´ì¦ˆ í¬ê¸°\n",
        "NOISE_REDUCTION = True  # ë…¸ì´ì¦ˆ ì œê±° ì—¬ë¶€\n",
        "NORMALIZE_COLOR = True  # ìƒ‰ìƒ ì •ê·œí™” ì—¬ë¶€\n",
        "\n",
        "# ë°ì´í„°ì…‹ ì„¸íŠ¸ ì„¤ì • (ì‹¤ì œ ë°ì´í„° êµ¬ì¡°ì— ë§ê²Œ ìˆ˜ì •)\n",
        "DATASET_SETS = [\"dataset_1\", \"dataset_2\", \"dataset_3\"]\n",
        "\n",
        "print(f\"ì›ë³¸ ë°ì´í„° ê²½ë¡œ: {RAW_DATA_DIR}\")\n",
        "print(f\"ì „ì²˜ë¦¬ ë°ì´í„° ì €ì¥ ê²½ë¡œ: {PROCESSED_DATA_DIR}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ì „ì²˜ë¦¬ í•¨ìˆ˜ ì •ì˜\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def remove_noise(image):\n",
        "    \"\"\"\n",
        "    ì´ë¯¸ì§€ ë…¸ì´ì¦ˆ ì œê±° (Non-local Means Denoising)\n",
        "    \n",
        "    Args:\n",
        "        image: numpy ë°°ì—´ ì´ë¯¸ì§€\n",
        "        \n",
        "    Returns:\n",
        "        denoised_image: ë…¸ì´ì¦ˆ ì œê±°ëœ ì´ë¯¸ì§€\n",
        "    \"\"\"\n",
        "    if len(image.shape) == 3:\n",
        "        # ì»¬ëŸ¬ ì´ë¯¸ì§€\n",
        "        denoised = cv2.fastNlMeansDenoisingColored(image, None, 10, 10, 7, 21)\n",
        "    else:\n",
        "        # ê·¸ë ˆì´ìŠ¤ì¼€ì¼ ì´ë¯¸ì§€\n",
        "        denoised = cv2.fastNlMeansDenoising(image, None, 10, 7, 21)\n",
        "    return denoised\n",
        "\n",
        "\n",
        "def normalize_color(image):\n",
        "    \"\"\"\n",
        "    ìƒ‰ìƒ ì •ê·œí™” (íˆìŠ¤í† ê·¸ë¨ ê· ë“±í™”)\n",
        "    \n",
        "    Args:\n",
        "        image: numpy ë°°ì—´ ì´ë¯¸ì§€\n",
        "        \n",
        "    Returns:\n",
        "        normalized_image: ì •ê·œí™”ëœ ì´ë¯¸ì§€\n",
        "    \"\"\"\n",
        "    if len(image.shape) == 3:\n",
        "        # ì»¬ëŸ¬ ì´ë¯¸ì§€ - YUV ìƒ‰ê³µê°„ìœ¼ë¡œ ë³€í™˜ í›„ Y ì±„ë„ë§Œ ì •ê·œí™”\n",
        "        yuv = cv2.cvtColor(image, cv2.COLOR_RGB2YUV)\n",
        "        yuv[:, :, 0] = cv2.equalizeHist(yuv[:, :, 0])\n",
        "        normalized = cv2.cvtColor(yuv, cv2.COLOR_YUV2RGB)\n",
        "    else:\n",
        "        # ê·¸ë ˆì´ìŠ¤ì¼€ì¼ ì´ë¯¸ì§€\n",
        "        normalized = cv2.equalizeHist(image)\n",
        "    return normalized\n",
        "\n",
        "\n",
        "def clean_filename(filename):\n",
        "    \"\"\"\n",
        "    íŒŒì¼ëª… ì •ë¦¬ (íŠ¹ìˆ˜ë¬¸ì ì œê±°, ê³µë°± ì²˜ë¦¬)\n",
        "    \n",
        "    Args:\n",
        "        filename: ì›ë³¸ íŒŒì¼ëª…\n",
        "        \n",
        "    Returns:\n",
        "        cleaned_filename: ì •ë¦¬ëœ íŒŒì¼ëª…\n",
        "    \"\"\"\n",
        "    # í™•ì¥ì ë¶„ë¦¬\n",
        "    name, ext = os.path.splitext(filename)\n",
        "    \n",
        "    # íŠ¹ìˆ˜ë¬¸ì ì œê±° ë° ê³µë°±ì„ ì–¸ë”ìŠ¤ì½”ì–´ë¡œ ë³€ê²½\n",
        "    cleaned = re.sub(r'[^a-zA-Z0-9ê°€-í£]', '_', name)\n",
        "    cleaned = re.sub(r'_+', '_', cleaned)  # ì—°ì†ëœ ì–¸ë”ìŠ¤ì½”ì–´ ì œê±°\n",
        "    cleaned = cleaned.strip('_')  # ì•ë’¤ ì–¸ë”ìŠ¤ì½”ì–´ ì œê±°\n",
        "    \n",
        "    return f\"{cleaned}{ext}\"\n",
        "\n",
        "\n",
        "def get_category_label(filepath):\n",
        "    \"\"\"\n",
        "    íŒŒì¼ ê²½ë¡œì—ì„œ ì¹´í…Œê³ ë¦¬ ë¼ë²¨ ì¶”ì¶œ\n",
        "    \n",
        "    Args:\n",
        "        filepath: íŒŒì¼ ê²½ë¡œ\n",
        "        \n",
        "    Returns:\n",
        "        category: ì¹´í…Œê³ ë¦¬ ë¼ë²¨ (ì˜ˆ: 'real', 'fake')\n",
        "    \"\"\"\n",
        "    # ê²½ë¡œì—ì„œ ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ (ì˜ˆ: data/raw/real/image.jpg -> 'real')\n",
        "    parts = Path(filepath).parts\n",
        "    \n",
        "    # 'real' ë˜ëŠ” 'fake' í‚¤ì›Œë“œ ì°¾ê¸° (ëŒ€ì†Œë¬¸ì êµ¬ë¶„ ì—†ìŒ)\n",
        "    for part in parts:\n",
        "        part_lower = part.lower()\n",
        "        if part_lower in ['real', 'authentic']:\n",
        "            return 'real'\n",
        "        elif part_lower in ['fake', 'ai', 'generated']:\n",
        "            return 'fake'\n",
        "    \n",
        "    # ê¸°ë³¸ê°’ (íŒŒì¼ëª…ì´ë‚˜ ê²½ë¡œì—ì„œ ì¶”ë¡  ë¶ˆê°€ëŠ¥í•œ ê²½ìš°)\n",
        "    return 'unknown'\n",
        "\n",
        "\n",
        "def preprocess_image(image_path, output_size=IMAGE_SIZE, \n",
        "                    remove_noise_flag=NOISE_REDUCTION, \n",
        "                    normalize_color_flag=NORMALIZE_COLOR):\n",
        "    \"\"\"\n",
        "    ë‹¨ì¼ ì´ë¯¸ì§€ ì „ì²˜ë¦¬ í•¨ìˆ˜\n",
        "    \n",
        "    Args:\n",
        "        image_path: ì´ë¯¸ì§€ íŒŒì¼ ê²½ë¡œ\n",
        "        output_size: ë¦¬ì‚¬ì´ì¦ˆí•  í¬ê¸° (width, height)\n",
        "        remove_noise_flag: ë…¸ì´ì¦ˆ ì œê±° ì—¬ë¶€\n",
        "        normalize_color_flag: ìƒ‰ìƒ ì •ê·œí™” ì—¬ë¶€\n",
        "        \n",
        "    Returns:\n",
        "        processed_image: ì „ì²˜ë¦¬ëœ PIL Image ê°ì²´\n",
        "    \"\"\"\n",
        "    # ì´ë¯¸ì§€ ë¡œë“œ\n",
        "    image = cv2.imread(str(image_path))\n",
        "    if image is None:\n",
        "        raise ValueError(f\"ì´ë¯¸ì§€ë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {image_path}\")\n",
        "    \n",
        "    # BGRì„ RGBë¡œ ë³€í™˜\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    \n",
        "    # ë…¸ì´ì¦ˆ ì œê±°\n",
        "    if remove_noise_flag:\n",
        "        image = remove_noise(image)\n",
        "    \n",
        "    # ìƒ‰ìƒ ì •ê·œí™”\n",
        "    if normalize_color_flag:\n",
        "        image = normalize_color(image)\n",
        "    \n",
        "    # ë¦¬ì‚¬ì´ì¦ˆ\n",
        "    image = cv2.resize(image, output_size, interpolation=cv2.INTER_LINEAR)\n",
        "    \n",
        "    # PIL Imageë¡œ ë³€í™˜\n",
        "    processed_image = Image.fromarray(image)\n",
        "    \n",
        "    return processed_image\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ë°ì´í„°ì…‹ë³„ ì „ì²˜ë¦¬ ì‹¤í–‰\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def process_dataset_set(set_name, raw_dir=RAW_DATA_DIR, processed_dir=PROCESSED_DATA_DIR):\n",
        "    \"\"\"\n",
        "    íŠ¹ì • ë°ì´í„°ì…‹ ì„¸íŠ¸ë¥¼ ì „ì²˜ë¦¬í•˜ì—¬ ì €ì¥\n",
        "    \n",
        "    Args:\n",
        "        set_name: ë°ì´í„°ì…‹ ì„¸íŠ¸ ì´ë¦„ (ì˜ˆ: 'set1')\n",
        "        raw_dir: ì›ë³¸ ë°ì´í„° ë””ë ‰í† ë¦¬\n",
        "        processed_dir: ì „ì²˜ë¦¬ëœ ë°ì´í„° ì €ì¥ ë””ë ‰í† ë¦¬\n",
        "    \"\"\"\n",
        "    # ì…ë ¥ ë° ì¶œë ¥ ê²½ë¡œ ì„¤ì •\n",
        "    set_raw_dir = raw_dir / set_name\n",
        "    set_processed_dir = processed_dir / f\"{set_name}_processed\"\n",
        "    \n",
        "    # ì›ë³¸ ë°ì´í„° ë””ë ‰í† ë¦¬ í™•ì¸\n",
        "    if not set_raw_dir.exists():\n",
        "        print(f\"âš ï¸  ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {set_raw_dir}\")\n",
        "        return\n",
        "    \n",
        "    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±\n",
        "    set_processed_dir.mkdir(parents=True, exist_ok=True)\n",
        "    \n",
        "    # ì¹´í…Œê³ ë¦¬ë³„ í†µê³„\n",
        "    category_stats = {}\n",
        "    \n",
        "    # ì´ë¯¸ì§€ íŒŒì¼ ì°¾ê¸°\n",
        "    image_extensions = {'.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.tif'}\n",
        "    image_files = []\n",
        "    \n",
        "    for ext in image_extensions:\n",
        "        image_files.extend(set_raw_dir.rglob(f\"*{ext}\"))\n",
        "        image_files.extend(set_raw_dir.rglob(f\"*{ext.upper()}\"))\n",
        "    \n",
        "    if len(image_files) == 0:\n",
        "        print(f\"âš ï¸  {set_name}ì—ì„œ ì´ë¯¸ì§€ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
        "        return\n",
        "    \n",
        "    print(f\"\\nğŸ“ {set_name} ì²˜ë¦¬ ì‹œì‘: {len(image_files)}ê°œ ì´ë¯¸ì§€\")\n",
        "    \n",
        "    # ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ë° ì €ì¥\n",
        "    processed_count = 0\n",
        "    failed_count = 0\n",
        "    \n",
        "    for img_path in tqdm(image_files, desc=f\"ì „ì²˜ë¦¬ ì¤‘ ({set_name})\"):\n",
        "        try:\n",
        "            # ì¹´í…Œê³ ë¦¬ ë¼ë²¨ ì¶”ì¶œ\n",
        "            category = get_category_label(img_path)\n",
        "            \n",
        "            # ì¹´í…Œê³ ë¦¬ë³„ ë””ë ‰í† ë¦¬ ìƒì„±\n",
        "            category_dir = set_processed_dir / category\n",
        "            category_dir.mkdir(parents=True, exist_ok=True)\n",
        "            \n",
        "            # íŒŒì¼ëª… ì •ë¦¬\n",
        "            cleaned_filename = clean_filename(img_path.name)\n",
        "            output_path = category_dir / cleaned_filename\n",
        "            \n",
        "            # ì¤‘ë³µ íŒŒì¼ëª… ì²˜ë¦¬\n",
        "            counter = 1\n",
        "            original_output_path = output_path\n",
        "            while output_path.exists():\n",
        "                name, ext = os.path.splitext(cleaned_filename)\n",
        "                output_path = category_dir / f\"{name}_{counter}{ext}\"\n",
        "                counter += 1\n",
        "            \n",
        "            # ì´ë¯¸ì§€ ì „ì²˜ë¦¬\n",
        "            processed_image = preprocess_image(img_path)\n",
        "            \n",
        "            # ì €ì¥\n",
        "            processed_image.save(output_path, quality=95)\n",
        "            \n",
        "            # í†µê³„ ì—…ë°ì´íŠ¸\n",
        "            if category not in category_stats:\n",
        "                category_stats[category] = 0\n",
        "            category_stats[category] += 1\n",
        "            processed_count += 1\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"\\nâŒ ì˜¤ë¥˜ ë°œìƒ ({img_path}): {str(e)}\")\n",
        "            failed_count += 1\n",
        "            continue\n",
        "    \n",
        "    # ê²°ê³¼ ì¶œë ¥\n",
        "    print(f\"\\nâœ… {set_name} ì „ì²˜ë¦¬ ì™„ë£Œ!\")\n",
        "    print(f\"   - ì„±ê³µ: {processed_count}ê°œ\")\n",
        "    print(f\"   - ì‹¤íŒ¨: {failed_count}ê°œ\")\n",
        "    print(f\"   - ì¹´í…Œê³ ë¦¬ë³„ í†µê³„:\")\n",
        "    for cat, count in category_stats.items():\n",
        "        print(f\"     * {cat}: {count}ê°œ\")\n",
        "    \n",
        "    # ë©”íƒ€ë°ì´í„° ì €ì¥\n",
        "    metadata = {\n",
        "        \"set_name\": set_name,\n",
        "        \"total_images\": len(image_files),\n",
        "        \"processed\": processed_count,\n",
        "        \"failed\": failed_count,\n",
        "        \"category_stats\": category_stats,\n",
        "        \"preprocessing_config\": {\n",
        "            \"image_size\": IMAGE_SIZE,\n",
        "            \"noise_removal\": NOISE_REDUCTION,\n",
        "            \"color_normalization\": NORMALIZE_COLOR\n",
        "        }\n",
        "    }\n",
        "    \n",
        "    metadata_path = set_processed_dir / \"metadata.json\"\n",
        "    with open(metadata_path, 'w', encoding='utf-8') as f:\n",
        "        json.dump(metadata, f, indent=2, ensure_ascii=False)\n",
        "    \n",
        "    print(f\"   - ë©”íƒ€ë°ì´í„° ì €ì¥: {metadata_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ì „ì²´ ë°ì´í„°ì…‹ ì „ì²˜ë¦¬ ì‹¤í–‰\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ëª¨ë“  ë°ì´í„°ì…‹ ì„¸íŠ¸ ì „ì²˜ë¦¬ ì‹¤í–‰\n",
        "print(\"=\" * 60)\n",
        "print(\"ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì‹œì‘\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for set_name in DATASET_SETS:\n",
        "    process_dataset_set(set_name)\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "print(\"\\nğŸ‰ ëª¨ë“  ë°ì´í„°ì…‹ ì „ì²˜ë¦¬ ì™„ë£Œ!\")\n",
        "print(f\"ì „ì²˜ë¦¬ëœ ë°ì´í„° ì €ì¥ ìœ„ì¹˜: {PROCESSED_DATA_DIR}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ì „ì²˜ë¦¬ ê²°ê³¼ í™•ì¸\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ì „ì²˜ë¦¬ëœ ë°ì´í„° êµ¬ì¡° í™•ì¸\n",
        "print(\"\\nğŸ“Š ì „ì²˜ë¦¬ëœ ë°ì´í„° êµ¬ì¡°:\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for set_name in DATASET_SETS:\n",
        "    set_processed_dir = PROCESSED_DATA_DIR / f\"{set_name}_processed\"\n",
        "    if set_processed_dir.exists():\n",
        "        print(f\"\\n{set_name}_processed/\")\n",
        "        for category_dir in sorted(set_processed_dir.iterdir()):\n",
        "            if category_dir.is_dir():\n",
        "                image_count = len(list(category_dir.glob(\"*.jpg\"))) + \\\n",
        "                             len(list(category_dir.glob(\"*.png\"))) + \\\n",
        "                             len(list(category_dir.glob(\"*.jpeg\")))\n",
        "                print(f\"  â”œâ”€â”€ {category_dir.name}/ ({image_count}ê°œ ì´ë¯¸ì§€)\")\n",
        "        \n",
        "        # ë©”íƒ€ë°ì´í„° í™•ì¸\n",
        "        metadata_path = set_processed_dir / \"metadata.json\"\n",
        "        if metadata_path.exists():\n",
        "            with open(metadata_path, 'r', encoding='utf-8') as f:\n",
        "                metadata = json.load(f)\n",
        "            print(f\"  â””â”€â”€ metadata.json (ì´ {metadata['processed']}ê°œ ì²˜ë¦¬ë¨)\")\n",
        "    else:\n",
        "        print(f\"\\n{set_name}_processed/ (ì¡´ì¬í•˜ì§€ ì•ŠìŒ)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ìƒ˜í”Œ ì´ë¯¸ì§€ ì‹œê°í™” (ì „ì²˜ë¦¬ ì „/í›„ ë¹„êµ)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def visualize_preprocessing_comparison(raw_path, processed_path, set_name=\"sample\"):\n",
        "    \"\"\"\n",
        "    ì „ì²˜ë¦¬ ì „/í›„ ì´ë¯¸ì§€ ë¹„êµ ì‹œê°í™”\n",
        "    \n",
        "    Args:\n",
        "        raw_path: ì›ë³¸ ì´ë¯¸ì§€ ê²½ë¡œ\n",
        "        processed_path: ì „ì²˜ë¦¬ëœ ì´ë¯¸ì§€ ê²½ë¡œ\n",
        "        set_name: ë°ì´í„°ì…‹ ì„¸íŠ¸ ì´ë¦„\n",
        "    \"\"\"\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
        "    \n",
        "    # ì›ë³¸ ì´ë¯¸ì§€\n",
        "    raw_img = Image.open(raw_path)\n",
        "    axes[0].imshow(raw_img)\n",
        "    axes[0].set_title(f\"ì›ë³¸ ì´ë¯¸ì§€\\ní¬ê¸°: {raw_img.size}\", fontsize=12)\n",
        "    axes[0].axis('off')\n",
        "    \n",
        "    # ì „ì²˜ë¦¬ëœ ì´ë¯¸ì§€\n",
        "    processed_img = Image.open(processed_path)\n",
        "    axes[1].imshow(processed_img)\n",
        "    axes[1].set_title(f\"ì „ì²˜ë¦¬ëœ ì´ë¯¸ì§€\\ní¬ê¸°: {processed_img.size}\", fontsize=12)\n",
        "    axes[1].axis('off')\n",
        "    \n",
        "    plt.suptitle(f\"{set_name} - ì „ì²˜ë¦¬ ì „/í›„ ë¹„êµ\", fontsize=14, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# ìƒ˜í”Œ ì´ë¯¸ì§€ ì°¾ê¸° ë° ì‹œê°í™”\n",
        "sample_found = False\n",
        "for set_name in DATASET_SETS:\n",
        "    set_raw_dir = RAW_DATA_DIR / set_name\n",
        "    set_processed_dir = PROCESSED_DATA_DIR / f\"{set_name}_processed\"\n",
        "    \n",
        "    if set_raw_dir.exists() and set_processed_dir.exists():\n",
        "        # ì²« ë²ˆì§¸ ì´ë¯¸ì§€ ì°¾ê¸°\n",
        "        raw_images = list(set_raw_dir.rglob(\"*.jpg\")) + list(set_raw_dir.rglob(\"*.png\"))\n",
        "        if raw_images:\n",
        "            raw_path = raw_images[0]\n",
        "            category = get_category_label(raw_path)\n",
        "            processed_category_dir = set_processed_dir / category\n",
        "            \n",
        "            if processed_category_dir.exists():\n",
        "                processed_images = list(processed_category_dir.glob(\"*.jpg\")) + \\\n",
        "                                 list(processed_category_dir.glob(\"*.png\"))\n",
        "                if processed_images:\n",
        "                    processed_path = processed_images[0]\n",
        "                    visualize_preprocessing_comparison(raw_path, processed_path, set_name)\n",
        "                    sample_found = True\n",
        "                    break\n",
        "\n",
        "if not sample_found:\n",
        "    print(\"âš ï¸  ì‹œê°í™”í•  ìƒ˜í”Œ ì´ë¯¸ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ë°ì´í„°ì…‹ í†µí•© ë° ë¶„í• \n",
        "\n",
        "ì „ì²˜ë¦¬ëœ 3ê°œ ë°ì´í„°ì…‹ì„ í•˜ë‚˜ë¡œ í•©ì¹˜ê³  train/val/testë¡œ ë¶„í• í•©ë‹ˆë‹¤.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def merge_datasets(processed_dir=PROCESSED_DATA_DIR, output_dir=None, exclude_unknown=True):\n",
        "    \"\"\"\n",
        "    ì „ì²˜ë¦¬ëœ ì—¬ëŸ¬ ë°ì´í„°ì…‹ì„ í•˜ë‚˜ë¡œ í†µí•©\n",
        "    \n",
        "    Args:\n",
        "        processed_dir: ì „ì²˜ë¦¬ëœ ë°ì´í„°ê°€ ìˆëŠ” ë””ë ‰í† ë¦¬\n",
        "        output_dir: í†µí•©ëœ ë°ì´í„°ë¥¼ ì €ì¥í•  ë””ë ‰í† ë¦¬ (ê¸°ë³¸ê°’: data/processed/all)\n",
        "        exclude_unknown: unknown ì¹´í…Œê³ ë¦¬ ì œì™¸ ì—¬ë¶€\n",
        "        \n",
        "    Returns:\n",
        "        merged_stats: í†µí•© í†µê³„ ì •ë³´\n",
        "    \"\"\"\n",
        "    if output_dir is None:\n",
        "        # data/processed/all/ ê²½ë¡œë¡œ ì„¤ì •\n",
        "        output_dir = processed_dir / \"all\"\n",
        "    else:\n",
        "        output_dir = Path(output_dir)\n",
        "    \n",
        "    output_dir.mkdir(parents=True, exist_ok=True)\n",
        "    \n",
        "    # í†µí•© í†µê³„\n",
        "    merged_stats = {}\n",
        "    total_images = 0\n",
        "    \n",
        "    # ëª¨ë“  ì „ì²˜ë¦¬ëœ ë°ì´í„°ì…‹ ì°¾ê¸°\n",
        "    processed_sets = [d for d in processed_dir.iterdir() \n",
        "                      if d.is_dir() and d.name.endswith(\"_processed\")]\n",
        "    \n",
        "    print(f\"ğŸ“¦ {len(processed_sets)}ê°œ ë°ì´í„°ì…‹ í†µí•© ì‹œì‘...\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    for processed_set in processed_sets:\n",
        "        set_name = processed_set.name\n",
        "        print(f\"\\nì²˜ë¦¬ ì¤‘: {set_name}\")\n",
        "        \n",
        "        # ì¹´í…Œê³ ë¦¬ë³„ë¡œ ì²˜ë¦¬\n",
        "        for category_dir in processed_set.iterdir():\n",
        "            if not category_dir.is_dir():\n",
        "                continue\n",
        "            \n",
        "            category = category_dir.name\n",
        "            \n",
        "            # unknown ì¹´í…Œê³ ë¦¬ ì œì™¸ ì˜µì…˜\n",
        "            if exclude_unknown and category == \"unknown\":\n",
        "                print(f\"  â­ï¸  {category} ì¹´í…Œê³ ë¦¬ ì œì™¸\")\n",
        "                continue\n",
        "            \n",
        "            # ì¶œë ¥ ì¹´í…Œê³ ë¦¬ ë””ë ‰í† ë¦¬ ìƒì„±\n",
        "            output_category_dir = output_dir / category\n",
        "            output_category_dir.mkdir(parents=True, exist_ok=True)\n",
        "            \n",
        "            # ì´ë¯¸ì§€ íŒŒì¼ ì°¾ê¸°\n",
        "            image_files = []\n",
        "            for ext in ['.jpg', '.jpeg', '.png', '.bmp']:\n",
        "                image_files.extend(category_dir.glob(f\"*{ext}\"))\n",
        "                image_files.extend(category_dir.glob(f\"*{ext.upper()}\"))\n",
        "            \n",
        "            # íŒŒì¼ ë³µì‚¬ (ì¤‘ë³µ ë°©ì§€ë¥¼ ìœ„í•´ ì›ë³¸ íŒŒì¼ëª…ì— ë°ì´í„°ì…‹ ì´ë¦„ ì¶”ê°€)\n",
        "            copied_count = 0\n",
        "            for img_file in image_files:\n",
        "                # íŒŒì¼ëª…ì— ë°ì´í„°ì…‹ ì´ë¦„ ì¶”ê°€í•˜ì—¬ ì¤‘ë³µ ë°©ì§€\n",
        "                new_filename = f\"{set_name}_{img_file.name}\"\n",
        "                output_path = output_category_dir / new_filename\n",
        "                \n",
        "                # ì´ë¯¸ ì¡´ì¬í•˜ëŠ” ê²½ìš° ìŠ¤í‚µ (ì¤‘ë³µ ë°©ì§€)\n",
        "                if not output_path.exists():\n",
        "                    shutil.copy2(img_file, output_path)\n",
        "                    copied_count += 1\n",
        "            \n",
        "            # í†µê³„ ì—…ë°ì´íŠ¸\n",
        "            if category not in merged_stats:\n",
        "                merged_stats[category] = 0\n",
        "            merged_stats[category] += copied_count\n",
        "            total_images += copied_count\n",
        "            \n",
        "            print(f\"  âœ… {category}: {copied_count}ê°œ ì´ë¯¸ì§€ ì¶”ê°€\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(f\"âœ… ë°ì´í„°ì…‹ í†µí•© ì™„ë£Œ!\")\n",
        "    print(f\"   ì´ ì´ë¯¸ì§€ ìˆ˜: {total_images}ê°œ\")\n",
        "    print(f\"   ì €ì¥ ìœ„ì¹˜: {output_dir}\")\n",
        "    print(f\"   ì¹´í…Œê³ ë¦¬ë³„ í†µê³„:\")\n",
        "    for cat, count in merged_stats.items():\n",
        "        print(f\"     - {cat}: {count}ê°œ\")\n",
        "    \n",
        "    # ë©”íƒ€ë°ì´í„° ì €ì¥\n",
        "    metadata = {\n",
        "        \"total_images\": total_images,\n",
        "        \"category_stats\": merged_stats,\n",
        "        \"merged_datasets\": [d.name for d in processed_sets],\n",
        "        \"exclude_unknown\": exclude_unknown\n",
        "    }\n",
        "    \n",
        "    metadata_path = output_dir / \"metadata.json\"\n",
        "    with open(metadata_path, 'w', encoding='utf-8') as f:\n",
        "        json.dump(metadata, f, indent=2, ensure_ascii=False)\n",
        "    \n",
        "    print(f\"\\n   ë©”íƒ€ë°ì´í„° ì €ì¥: {metadata_path}\")\n",
        "    \n",
        "    return merged_stats, output_dir\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def split_merged_dataset(merged_dir, \n",
        "                        train_ratio=0.7, \n",
        "                        val_ratio=0.15, \n",
        "                        test_ratio=0.15,\n",
        "                        random_state=42,\n",
        "                        stratify=True):\n",
        "    \"\"\"\n",
        "    í†µí•©ëœ ë°ì´í„°ì…‹ì„ train/val/testë¡œ ë¶„í• \n",
        "    \n",
        "    Args:\n",
        "        merged_dir: í†µí•©ëœ ë°ì´í„°ê°€ ìˆëŠ” ë””ë ‰í† ë¦¬\n",
        "        train_ratio: í›ˆë ¨ ì„¸íŠ¸ ë¹„ìœ¨\n",
        "        val_ratio: ê²€ì¦ ì„¸íŠ¸ ë¹„ìœ¨\n",
        "        test_ratio: í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ ë¹„ìœ¨\n",
        "        random_state: ëœë¤ ì‹œë“œ\n",
        "        stratify: í´ë˜ìŠ¤ë³„ ë¹„ìœ¨ ìœ ì§€ ì—¬ë¶€\n",
        "        \n",
        "    Returns:\n",
        "        split_stats: ë¶„í•  í†µê³„ ì •ë³´\n",
        "    \"\"\"\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    import random\n",
        "    \n",
        "    assert abs(train_ratio + val_ratio + test_ratio - 1.0) < 1e-6, \\\n",
        "        \"ë¹„ìœ¨ì˜ í•©ì€ 1.0ì´ì–´ì•¼ í•©ë‹ˆë‹¤\"\n",
        "    \n",
        "    merged_path = Path(merged_dir)\n",
        "    output_base = merged_path.parent.parent  # data/ ë””ë ‰í† ë¦¬\n",
        "    \n",
        "    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±\n",
        "    train_dir = output_base / \"train\"\n",
        "    val_dir = output_base / \"val\"\n",
        "    test_dir = output_base / \"test\"\n",
        "    \n",
        "    split_stats = {\n",
        "        \"train\": {},\n",
        "        \"val\": {},\n",
        "        \"test\": {}\n",
        "    }\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"ğŸ“Š ë°ì´í„°ì…‹ ë¶„í•  ì‹œì‘...\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"   ë¹„ìœ¨: Train {train_ratio*100:.0f}% / Val {val_ratio*100:.0f}% / Test {test_ratio*100:.0f}%\")\n",
        "    print(f\"   Random State: {random_state}\")\n",
        "    print(f\"   Stratify: {stratify}\")\n",
        "    print(\"-\" * 60)\n",
        "    \n",
        "    # í´ë˜ìŠ¤ë³„ë¡œ ë¶„í• \n",
        "    for category_dir in sorted(merged_path.iterdir()):\n",
        "        if not category_dir.is_dir():\n",
        "            continue\n",
        "        \n",
        "        category = category_dir.name\n",
        "        \n",
        "        # ì´ë¯¸ì§€ íŒŒì¼ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°\n",
        "        image_files = []\n",
        "        for ext in ['.jpg', '.jpeg', '.png', '.bmp']:\n",
        "            image_files.extend(category_dir.glob(f\"*{ext}\"))\n",
        "            image_files.extend(category_dir.glob(f\"*{ext.upper()}\"))\n",
        "        \n",
        "        image_files = sorted(image_files)  # ì¬í˜„ì„±ì„ ìœ„í•´ ì •ë ¬\n",
        "        total_files = len(image_files)\n",
        "        \n",
        "        if total_files == 0:\n",
        "            print(f\"âš ï¸  {category}: ì´ë¯¸ì§€ íŒŒì¼ ì—†ìŒ\")\n",
        "            continue\n",
        "        \n",
        "        # ëœë¤ ì‹œë“œ ì„¤ì •\n",
        "        random.seed(random_state)\n",
        "        np.random.seed(random_state)\n",
        "        \n",
        "        # train/val/testë¡œ ë¶„í• \n",
        "        # ë¨¼ì € trainê³¼ ë‚˜ë¨¸ì§€ë¡œ ë¶„í• \n",
        "        train_files, temp_files = train_test_split(\n",
        "            image_files, \n",
        "            test_size=(val_ratio + test_ratio), \n",
        "            random_state=random_state,\n",
        "            shuffle=True\n",
        "        )\n",
        "        \n",
        "        # valê³¼ testë¡œ ë¶„í• \n",
        "        val_files, test_files = train_test_split(\n",
        "            temp_files,\n",
        "            test_size=test_ratio / (val_ratio + test_ratio),\n",
        "            random_state=random_state,\n",
        "            shuffle=True\n",
        "        )\n",
        "        \n",
        "        # íŒŒì¼ ë³µì‚¬\n",
        "        for split_name, file_list, split_dir in [\n",
        "            (\"train\", train_files, train_dir),\n",
        "            (\"val\", val_files, val_dir),\n",
        "            (\"test\", test_files, test_dir)\n",
        "        ]:\n",
        "            output_category_dir = split_dir / category\n",
        "            output_category_dir.mkdir(parents=True, exist_ok=True)\n",
        "            \n",
        "            for file_path in file_list:\n",
        "                shutil.copy2(file_path, output_category_dir / file_path.name)\n",
        "            \n",
        "            split_stats[split_name][category] = len(file_list)\n",
        "        \n",
        "        print(f\"\\nğŸ“ {category}:\")\n",
        "        print(f\"   Train: {len(train_files)}ê°œ ({len(train_files)/total_files*100:.1f}%)\")\n",
        "        print(f\"   Val:   {len(val_files)}ê°œ ({len(val_files)/total_files*100:.1f}%)\")\n",
        "        print(f\"   Test:  {len(test_files)}ê°œ ({len(test_files)/total_files*100:.1f}%)\")\n",
        "    \n",
        "    # ì „ì²´ í†µê³„ ì¶œë ¥\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"âœ… ë°ì´í„°ì…‹ ë¶„í•  ì™„ë£Œ!\")\n",
        "    print(\"-\" * 60)\n",
        "    \n",
        "    total_train = sum(split_stats[\"train\"].values())\n",
        "    total_val = sum(split_stats[\"val\"].values())\n",
        "    total_test = sum(split_stats[\"test\"].values())\n",
        "    total_all = total_train + total_val + total_test\n",
        "    \n",
        "    print(f\"\\nğŸ“Š ì „ì²´ í†µê³„:\")\n",
        "    print(f\"   Train: {total_train}ê°œ ({total_train/total_all*100:.1f}%)\")\n",
        "    print(f\"   Val:   {total_val}ê°œ ({total_val/total_all*100:.1f}%)\")\n",
        "    print(f\"   Test:  {total_test}ê°œ ({total_test/total_all*100:.1f}%)\")\n",
        "    print(f\"   ì´ê³„:  {total_all}ê°œ\")\n",
        "    \n",
        "    print(f\"\\nğŸ“‚ ì €ì¥ ìœ„ì¹˜:\")\n",
        "    print(f\"   Train: {train_dir}\")\n",
        "    print(f\"   Val:   {val_dir}\")\n",
        "    print(f\"   Test:  {test_dir}\")\n",
        "    \n",
        "    # ë¶„í•  ë©”íƒ€ë°ì´í„° ì €ì¥\n",
        "    split_metadata = {\n",
        "        \"split_ratios\": {\n",
        "            \"train\": train_ratio,\n",
        "            \"val\": val_ratio,\n",
        "            \"test\": test_ratio\n",
        "        },\n",
        "        \"random_state\": random_state,\n",
        "        \"stratify\": stratify,\n",
        "        \"category_stats\": split_stats,\n",
        "        \"total_stats\": {\n",
        "            \"train\": total_train,\n",
        "            \"val\": total_val,\n",
        "            \"test\": total_test,\n",
        "            \"total\": total_all\n",
        "        }\n",
        "    }\n",
        "    \n",
        "    metadata_path = output_base / \"split_metadata.json\"\n",
        "    with open(metadata_path, 'w', encoding='utf-8') as f:\n",
        "        json.dump(split_metadata, f, indent=2, ensure_ascii=False)\n",
        "    \n",
        "    print(f\"\\n   ë¶„í•  ë©”íƒ€ë°ì´í„° ì €ì¥: {metadata_path}\")\n",
        "    \n",
        "    return split_stats\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## í†µí•© ë° ë¶„í•  ì‹¤í–‰\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1ë‹¨ê³„: ë°ì´í„°ì…‹ í†µí•©\n",
        "print(\"ğŸ”„ 1ë‹¨ê³„: ë°ì´í„°ì…‹ í†µí•©\")\n",
        "print(\"=\" * 60)\n",
        "merged_stats, merged_dir = merge_datasets(\n",
        "    processed_dir=PROCESSED_DATA_DIR,\n",
        "    exclude_unknown=True  # unknown ì¹´í…Œê³ ë¦¬ëŠ” ì œì™¸\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2ë‹¨ê³„: train/val/test ë¶„í• \n",
        "print(\"\\nğŸ”„ 2ë‹¨ê³„: ë°ì´í„°ì…‹ ë¶„í• \")\n",
        "split_stats = split_merged_dataset(\n",
        "    merged_dir=merged_dir,\n",
        "    train_ratio=0.7,   # 70% í›ˆë ¨\n",
        "    val_ratio=0.15,    # 15% ê²€ì¦\n",
        "    test_ratio=0.15,   # 15% í…ŒìŠ¤íŠ¸\n",
        "    random_state=42,   # ì¬í˜„ì„±ì„ ìœ„í•œ ì‹œë“œ\n",
        "    stratify=True      # í´ë˜ìŠ¤ë³„ ë¹„ìœ¨ ìœ ì§€\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ë¶„í•  ê²°ê³¼ í™•ì¸\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ë¶„í• ëœ ë°ì´í„° êµ¬ì¡° í™•ì¸\n",
        "print(\"\\nğŸ“Š ìµœì¢… ë°ì´í„° êµ¬ì¡°:\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "data_base = Path(\"../data\")\n",
        "for split_name in [\"train\", \"val\", \"test\"]:\n",
        "    split_dir = data_base / split_name\n",
        "    if split_dir.exists():\n",
        "        print(f\"\\n{split_name}/\")\n",
        "        total = 0\n",
        "        for category_dir in sorted(split_dir.iterdir()):\n",
        "            if category_dir.is_dir():\n",
        "                image_count = len(list(category_dir.glob(\"*.jpg\"))) + \\\n",
        "                             len(list(category_dir.glob(\"*.png\"))) + \\\n",
        "                             len(list(category_dir.glob(\"*.jpeg\")))\n",
        "                total += image_count\n",
        "                print(f\"  â”œâ”€â”€ {category_dir.name}/ ({image_count:,}ê°œ)\")\n",
        "        print(f\"  â””â”€â”€ ì´ê³„: {total:,}ê°œ\")\n",
        "    else:\n",
        "        print(f\"\\n{split_name}/ (ì¡´ì¬í•˜ì§€ ì•ŠìŒ)\")\n",
        "\n",
        "# í†µí•© ë°ì´í„° í™•ì¸\n",
        "all_dir = data_base / \"processed\" / \"all\"\n",
        "if all_dir.exists():\n",
        "    print(f\"\\nprocessed/all/\")\n",
        "    total = 0\n",
        "    for category_dir in sorted(all_dir.iterdir()):\n",
        "        if category_dir.is_dir():\n",
        "            image_count = len(list(category_dir.glob(\"*.jpg\"))) + \\\n",
        "                         len(list(category_dir.glob(\"*.png\"))) + \\\n",
        "                         len(list(category_dir.glob(\"*.jpeg\")))\n",
        "            total += image_count\n",
        "            print(f\"  â”œâ”€â”€ {category_dir.name}/ ({image_count:,}ê°œ)\")\n",
        "    print(f\"  â””â”€â”€ ì´ê³„: {total:,}ê°œ\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ë°ì´í„° ë¶ˆê· í˜• í™•ì¸ ë° ì‹œê°í™”\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ë°ì´í„° ë¶ˆê· í˜• ì‹œê°í™”\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# í†µí•© ë°ì´í„° í†µê³„\n",
        "categories = list(merged_stats.keys())\n",
        "counts = list(merged_stats.values())\n",
        "\n",
        "# ë¶„í• ë³„ í†µê³„\n",
        "train_counts = [split_stats[\"train\"].get(cat, 0) for cat in categories]\n",
        "val_counts = [split_stats[\"val\"].get(cat, 0) for cat in categories]\n",
        "test_counts = [split_stats[\"test\"].get(cat, 0) for cat in categories]\n",
        "\n",
        "# ì‹œê°í™”\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# 1. ì „ì²´ í†µí•© ë°ì´í„° ë¶„í¬\n",
        "axes[0].bar(categories, counts, color=['#3498db', '#e74c3c'])\n",
        "axes[0].set_title('í†µí•© ë°ì´í„°ì…‹ ë¶„í¬', fontsize=14, fontweight='bold')\n",
        "axes[0].set_ylabel('ì´ë¯¸ì§€ ìˆ˜', fontsize=12)\n",
        "axes[0].grid(axis='y', alpha=0.3)\n",
        "for i, (cat, count) in enumerate(zip(categories, counts)):\n",
        "    axes[0].text(i, count, f'{count:,}', ha='center', va='bottom', fontsize=10)\n",
        "\n",
        "# 2. Train/Val/Test ë¶„í•  ë¶„í¬\n",
        "x = np.arange(len(categories))\n",
        "width = 0.25\n",
        "\n",
        "axes[1].bar(x - width, train_counts, width, label='Train', color='#2ecc71')\n",
        "axes[1].bar(x, val_counts, width, label='Val', color='#f39c12')\n",
        "axes[1].bar(x + width, test_counts, width, label='Test', color='#9b59b6')\n",
        "\n",
        "axes[1].set_title('Train/Val/Test ë¶„í•  ë¶„í¬', fontsize=14, fontweight='bold')\n",
        "axes[1].set_ylabel('ì´ë¯¸ì§€ ìˆ˜', fontsize=12)\n",
        "axes[1].set_xticks(x)\n",
        "axes[1].set_xticklabels(categories)\n",
        "axes[1].legend()\n",
        "axes[1].grid(axis='y', alpha=0.3)\n",
        "\n",
        "# ê°’ í‘œì‹œ\n",
        "for i, (cat, train, val, test) in enumerate(zip(categories, train_counts, val_counts, test_counts)):\n",
        "    axes[1].text(i - width, train, f'{train:,}', ha='center', va='bottom', fontsize=8)\n",
        "    axes[1].text(i, val, f'{val:,}', ha='center', va='bottom', fontsize=8)\n",
        "    axes[1].text(i + width, test, f'{test:,}', ha='center', va='bottom', fontsize=8)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ë¶ˆê· í˜• ë¹„ìœ¨ ê³„ì‚°\n",
        "if len(counts) >= 2:\n",
        "    imbalance_ratio = max(counts) / min(counts)\n",
        "    print(f\"\\nğŸ“ˆ ë°ì´í„° ë¶ˆê· í˜• ë¹„ìœ¨: {imbalance_ratio:.2f}:1\")\n",
        "    print(f\"   (ê°€ì¥ ë§ì€ í´ë˜ìŠ¤ / ê°€ì¥ ì ì€ í´ë˜ìŠ¤)\")\n",
        "    \n",
        "    if imbalance_ratio > 3:\n",
        "        print(\"   âš ï¸  ë°ì´í„° ë¶ˆê· í˜•ì´ í½ë‹ˆë‹¤. ë°ì´í„° ì¦ê°•(data augmentation)ì„ ê¶Œì¥í•©ë‹ˆë‹¤.\")\n",
        "    else:\n",
        "        print(\"   âœ… ë°ì´í„° ë¶ˆê· í˜•ì´ ì ì ˆí•œ ìˆ˜ì¤€ì…ë‹ˆë‹¤.\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
