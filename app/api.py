"""
FastAPI ë°±ì—”ë“œ API ì„œë²„
AI ì´ë¯¸ì§€ íƒì§€ë¥¼ ìœ„í•œ RESTful API ì œê³µ
"""
from fastapi import FastAPI, File, UploadFile, HTTPException, Query
from fastapi.responses import JSONResponse
from fastapi.middleware.cors import CORSMiddleware
from fastapi.openapi.docs import get_swagger_ui_html
import torch
from PIL import Image
import io
import tempfile
import os
from pathlib import Path
import sys
from typing import Optional
from pydantic import BaseModel

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ ê²½ë¡œì— ì¶”ê°€
sys.path.insert(0, str(Path(__file__).parent.parent))

from src.inference.inference import (
    load_model_for_inference,
    predict_single_image
)

# FastAPI ì•± ìƒì„±
app = FastAPI(
    title="AI Image Detector API",
    description="AI ìƒì„± ì´ë¯¸ì§€ì™€ ì‹¤ì œ ì´ë¯¸ì§€ë¥¼ êµ¬ë¶„í•˜ëŠ” ë”¥ëŸ¬ë‹ ê¸°ë°˜ API",
    version="1.0.0",
    docs_url="/docs",
    redoc_url="/redoc"
)

# CORS ì„¤ì • (ì›¹ì—ì„œ ì ‘ê·¼ ê°€ëŠ¥í•˜ë„ë¡)
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # í”„ë¡œë•ì…˜ì—ì„œëŠ” íŠ¹ì • ë„ë©”ì¸ë§Œ í—ˆìš©
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ì „ì—­ ë³€ìˆ˜
cnn_model = None
vit_model = None
cnn_device = None
vit_device = None
class_names = ["Real", "AI"]


class PredictionResponse(BaseModel):
    """ì˜ˆì¸¡ ì‘ë‹µ ëª¨ë¸"""
    image_path: str
    predicted_class: str
    predicted_class_idx: int
    confidence: float
    probabilities: dict
    is_ai: bool
    model_type: str


@app.on_event("startup")
async def load_models_on_startup():
    """ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹œì‘ ì‹œ ëª¨ë¸ ë¡œë“œ"""
    global cnn_model, vit_model, cnn_device, vit_device
    
    print("ğŸš€ ëª¨ë¸ ë¡œë“œ ì‹œì‘...")
    
    # CNN ëª¨ë¸ ë¡œë“œ
    cnn_checkpoint = Path('experiments/checkpoints/CNN_resnet18_best.pth')
    if cnn_checkpoint.exists():
        try:
            cnn_device = 'cuda' if torch.cuda.is_available() else 'mps' if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available() else 'cpu'
            cnn_model, _ = load_model_for_inference(
                checkpoint_path=cnn_checkpoint,
                model_type='cnn',
                model_name='resnet18',
                num_classes=2,
                device=cnn_device
            )
            print("âœ… CNN ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
        except Exception as e:
            print(f"âŒ CNN ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
    else:
        print(f"âš ï¸ CNN ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ì—†ìŒ: {cnn_checkpoint}")
    
    # ViT ëª¨ë¸ ë¡œë“œ
    vit_checkpoint = Path('experiments/checkpoints/ViT_vit_base_best.pth')
    if vit_checkpoint.exists():
        try:
            vit_device = 'cuda' if torch.cuda.is_available() else 'mps' if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available() else 'cpu'
            vit_model, _ = load_model_for_inference(
                checkpoint_path=vit_checkpoint,
                model_type='vit',
                model_name='vit_base',
                num_classes=2,
                device=vit_device
            )
            print("âœ… ViT ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
        except Exception as e:
            print(f"âŒ ViT ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
    else:
        print(f"âš ï¸ ViT ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ì—†ìŒ: {vit_checkpoint}")
    
    print("ğŸ‰ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")


@app.get("/")
async def root():
    """ë£¨íŠ¸ ì—”ë“œí¬ì¸íŠ¸"""
    return {
        "message": "AI Image Detector API",
        "status": "running",
        "version": "1.0.0",
        "endpoints": {
            "health": "/health",
            "predict": "/predict",
            "docs": "/docs"
        }
    }


@app.get("/health")
async def health_check():
    """í—¬ìŠ¤ ì²´í¬ ì—”ë“œí¬ì¸íŠ¸"""
    models_status = {
        "CNN": cnn_model is not None,
        "ViT": vit_model is not None
    }
    
    return {
        "status": "healthy",
        "models": models_status,
        "devices": {
            "CNN": str(cnn_device) if cnn_device else None,
            "ViT": str(vit_device) if vit_device else None
        }
    }


@app.post("/predict", response_model=PredictionResponse)
async def predict_image(
    file: UploadFile = File(..., description="ë¶„ì„í•  ì´ë¯¸ì§€ íŒŒì¼"),
    model_type: str = Query("cnn", description="ì‚¬ìš©í•  ëª¨ë¸ íƒ€ì… (cnn ë˜ëŠ” vit)")
):
    """
    ì´ë¯¸ì§€ ì—…ë¡œë“œ ë° ì˜ˆì¸¡
    
    Args:
        file: ì—…ë¡œë“œëœ ì´ë¯¸ì§€ íŒŒì¼ (jpg, png, jpeg ë“±)
        model_type: ì‚¬ìš©í•  ëª¨ë¸ íƒ€ì… ('cnn' ë˜ëŠ” 'vit')
        
    Returns:
        PredictionResponse: ì˜ˆì¸¡ ê²°ê³¼ (í´ë˜ìŠ¤, ì‹ ë¢°ë„, í™•ë¥  ë“±)
    """
    # ëª¨ë¸ ì„ íƒ
    if model_type.lower() == "cnn":
        model = cnn_model
        device = cnn_device
        model_name = "CNN"
    elif model_type.lower() == "vit":
        model = vit_model
        device = vit_device
        model_name = "ViT"
    else:
        raise HTTPException(
            status_code=400,
            detail=f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë¸ íƒ€ì…: {model_type}. 'cnn' ë˜ëŠ” 'vit'ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”."
        )
    
    if model is None:
        raise HTTPException(
            status_code=503,
            detail=f"{model_name} ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”."
        )
    
    # ì´ë¯¸ì§€ íŒŒì¼ ê²€ì¦
    if not file.content_type or not file.content_type.startswith("image/"):
        raise HTTPException(
            status_code=400,
            detail="ì´ë¯¸ì§€ íŒŒì¼ë§Œ ì—…ë¡œë“œ ê°€ëŠ¥í•©ë‹ˆë‹¤. ì§€ì› í˜•ì‹: jpg, png, jpeg, bmp"
        )
    
    # ì„ì‹œ íŒŒì¼ ìƒì„±
    temp_file = None
    try:
        # ì´ë¯¸ì§€ ì½ê¸° ë° ê²€ì¦
        image_bytes = await file.read()
        image = Image.open(io.BytesIO(image_bytes)).convert('RGB')
        
        # ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥
        temp_file = tempfile.NamedTemporaryFile(delete=False, suffix='.jpg')
        image.save(temp_file.name)
        temp_path = temp_file.name
        
        # ì˜ˆì¸¡ ìˆ˜í–‰
        result = predict_single_image(
            model=model,
            image_path=temp_path,
            device=device,
            class_names=class_names
        )
        
        # ëª¨ë¸ íƒ€ì… ì¶”ê°€
        result['model_type'] = model_name
        
        return PredictionResponse(**result)
    
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"ì˜ˆì¸¡ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
        )
    
    finally:
        # ì„ì‹œ íŒŒì¼ ì‚­ì œ
        if temp_file and os.path.exists(temp_file.name):
            os.unlink(temp_file.name)


@app.post("/predict/batch")
async def predict_batch_images(
    files: list[UploadFile] = File(..., description="ë¶„ì„í•  ì´ë¯¸ì§€ íŒŒì¼ ë¦¬ìŠ¤íŠ¸"),
    model_type: str = Query("cnn", description="ì‚¬ìš©í•  ëª¨ë¸ íƒ€ì… (cnn ë˜ëŠ” vit)")
):
    """
    ì—¬ëŸ¬ ì´ë¯¸ì§€ì— ëŒ€í•œ ë°°ì¹˜ ì˜ˆì¸¡
    
    Args:
        files: ì—…ë¡œë“œëœ ì´ë¯¸ì§€ íŒŒì¼ ë¦¬ìŠ¤íŠ¸
        model_type: ì‚¬ìš©í•  ëª¨ë¸ íƒ€ì… ('cnn' ë˜ëŠ” 'vit')
        
    Returns:
        List[PredictionResponse]: ì˜ˆì¸¡ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
    """
    from src.inference.batch_inference import batch_predict
    
    # ëª¨ë¸ ì„ íƒ
    if model_type.lower() == "cnn":
        model = cnn_model
        device = cnn_device
        model_name = "CNN"
    elif model_type.lower() == "vit":
        model = vit_model
        device = vit_device
        model_name = "ViT"
    else:
        raise HTTPException(
            status_code=400,
            detail=f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë¸ íƒ€ì…: {model_type}"
        )
    
    if model is None:
        raise HTTPException(
            status_code=503,
            detail=f"{model_name} ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
        )
    
    # ì„ì‹œ ë””ë ‰í† ë¦¬ ìƒì„±
    temp_dir = tempfile.mkdtemp()
    temp_paths = []
    
    try:
        # ì´ë¯¸ì§€ íŒŒì¼ ì €ì¥
        for idx, file in enumerate(files):
            if not file.content_type or not file.content_type.startswith("image/"):
                continue
            
            image_bytes = await file.read()
            image = Image.open(io.BytesIO(image_bytes)).convert('RGB')
            
            temp_path = os.path.join(temp_dir, f"image_{idx}.jpg")
            image.save(temp_path)
            temp_paths.append(temp_path)
        
        if not temp_paths:
            raise HTTPException(status_code=400, detail="ìœ íš¨í•œ ì´ë¯¸ì§€ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        
        # ë°°ì¹˜ ì˜ˆì¸¡
        results = batch_predict(
            model=model,
            image_paths=temp_paths,
            device=device,
            batch_size=min(32, len(temp_paths)),
            class_names=class_names,
            num_workers=0,
            show_progress=False
        )
        
        # ëª¨ë¸ íƒ€ì… ì¶”ê°€
        for result in results:
            result['model_type'] = model_name
        
        return [PredictionResponse(**r) for r in results]
    
    finally:
        # ì„ì‹œ íŒŒì¼ ë° ë””ë ‰í† ë¦¬ ì‚­ì œ
        import shutil
        if os.path.exists(temp_dir):
            shutil.rmtree(temp_dir)


@app.get("/models")
async def get_available_models():
    """ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ì¡°íšŒ"""
    return {
        "available_models": {
            "CNN": cnn_model is not None,
            "ViT": vit_model is not None
        },
        "default": "cnn"
    }


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(
        app,
        host="0.0.0.0",
        port=8000,
        log_level="info"
    )

